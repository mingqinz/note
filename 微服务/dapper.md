## 设计目标

- 低开销：在高性能场景，千分之一的请求数据也能提供足够的信息
- 应用透明
- 规模可扩展
- 低延迟
- 无处不在部署
- 持续监控



## 调用链

参考 Google Dapper 论文实现，为每个请求都 生成一个全局唯一的 traceid，端到端透传到上 下游所有节点，每一层生成一个 spanid，通过 traceid 将不同系统孤立的调用日志和异常信息 串联一起，通过 spanid 和 level 表达节点的父 子关系。

• Tree

• Span
• Annotation



## 跟踪采样

• 固定采样，1/1024:

这个简单的方案是对我们的高吞吐量的线上服务来说是非常有用，因为那些感兴趣的事件(在大吞吐量 的情况下)仍然很有可能经常出现，并且通常足以被捕捉到。然而，在较低的采样率和较低的传输负载下 可能会导致错过重要事件，而想用较高的采样率就需要能接受的性能损耗。对于这样的系统的解决方案 就是覆盖默认的采样率，这需要手动干预的，这种情况是我们试图避免在 Dapper 中出现的。

• 应对积极采样:

我们理解为单位时间期望采集样本的条目，在高 QPS 下，采样率自然下降，在低 QPS 下，采样率自 然增加;比如1s内某个接口采集1条。



• 二级采样:

容器节点数量多，即使使用积极采样仍然会导致采样样本非常多，所以需要控制写入中央仓库的数据 的总规模，利用所有 span 都来自一个特定的跟踪并分享同一个 traceid 这个事实，虽然这些 span 有可 能横跨了数千个主机。

对于在收集系统中的每一个 span，我们用hash算法把 traceid 转成一个标量Z ，这里0<=Z<=1，我们选 择了运行期采样率，这样就可以优雅的去掉我们无法写入到仓库中的多余数据，我们还可以通过调节收 集系统中的二级采样率系数来调整这个运行期采样率，最终我们通过后端存储压力把策略下发给 agent 采集系统，实现精准的二级采样。

• 下游采样:
 越被依赖多的服务，网关层使用积极采样以后，对于 downstream 的服务采样率仍然很高。



